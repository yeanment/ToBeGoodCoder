{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Recitation0B_Fundamentals_of_Numpy_&_Pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.6 64-bit ('pytorch': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "b83584b0a2f8e12a0e4f5f74982b30364b9f18267f29dee29c19a7ed48d2ebdc"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fundamentals of Numpy & PyTorch\n",
        "---"
      ],
      "metadata": {
        "id": "Uc0IXcXq7pEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Contents**\n",
        "\n",
        "*Note, the contents is based on the [blog](https://rickwierenga.com/blog/machine%20learning/numpy-vs-pytorch-linalg.html).*\n",
        "\n",
        "1. Setting Up Numpy & Pytorch\n",
        "2. Basic Numpy Arrays & Pytorch Tensors\n",
        "  2a. Example of Numpy Arrays and Pytorch Tensors\n",
        "  2b. Pivoting Data\n",
        "  2c. Combining Data\n",
        "  2d. Mathematical operations\n",
        "3. Torch GPU Operation"
      ],
      "metadata": {
        "id": "Y5enyRdM5to8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Setting up Numpy and Pytorch**"
      ],
      "metadata": {
        "id": "2Z6Qy7hH2iEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# You can set install NumPy/torch by using the following command\n",
        "# !conda install numpy\n",
        "# !conda install pytorch\n",
        "# If you want to check the version of numpy/torch you are using \n",
        "# or if you want to conifrm if it is installed in your system:\n",
        "!conda list numpy \n",
        "!conda list torch "
      ],
      "outputs": [],
      "metadata": {
        "id": "Jh6NixenhdBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31f211e-f8d8-4afd-febd-6e60b93f1435"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "# Set random seed to 0 to ensure identical results in each run.\n",
        "torch.manual_seed(0) \n",
        "np.random.seed(0) "
      ],
      "outputs": [],
      "metadata": {
        "id": "BAGQtfHnhmbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Basic Numpy Arrays & Pytorch Tensors**"
      ],
      "metadata": {
        "id": "K2pbKR6Ui1Av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In NumPy, an array is a data structure that stores values of same data type, \n",
        "while the list in Python stores data of different types.\n",
        "Simimlar with Numppy array, the tensor in torch is a multi-dimensional matrix \n",
        "containing elements of a single data type."
      ],
      "metadata": {
        "id": "rqmJv0L7j8Nd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2a. Example of Numpy Arrays and Pytorch Tensors**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Initializes a numpy array of zeroes, with size (3,4)\n",
        "new_array1 = np.zeros((3, 4, 5))\n",
        "# Initializes a numpy array of ones, with size (3,4)\n",
        "new_array2 = np.ones((3, 4))\n",
        "# Initializes a random numpy array with standard normal distribution\n",
        "new_array2 = np.random.randn(3, 4) \n",
        "# Initializes a random numpy array with randomly distributed integers in [0, 3)\n",
        "new_array5 = np.random.randint(3, size = (4, 5)) \n",
        "\n",
        "\n",
        "# Initializes a torch tensor of zeroes, with size (5,3)\n",
        "new_tensor1 = torch.zeros(size=(5,3))\n",
        "# Initializes a torch tensor of ones, with size (5,3)\n",
        "new_tensor2 = torch.ones(size=(5,3))\n",
        "# Returns a 2-D tensor with ones on the diagonal and zeros elsewhere\n",
        "new_tensor3 = torch.eye(3)\n",
        "# Returns a tensor with random numbers from uniform distribution on [0, 1)\n",
        "new_tensor4 = torch.rand(size=(3,4))\n",
        "# Returns a 1-D tensor with values from the interval [start, end) taken with \n",
        "# common difference step beginning from start\n",
        "new_tensor5 = torch.arange(start=-3, end=9, step=2)\n",
        "\n",
        "\n",
        "# List - Numpy Array - Torch Tensor conversion\n",
        "# list -> array\n",
        "list1 = [1, 2, 3, 4, 5]\n",
        "list_toarray = np.array(list1) # print(type(list_toarray))\n",
        "# list -> tensor\n",
        "list2 = [6, 7, 8, 9, 10]\n",
        "list_totensor = torch.tensor(list2) # print(type(list_totensor))\n",
        "# array -> tensor\n",
        "nparray1 = np.array([1,2,3,4])\n",
        "array_totensor = torch.from_numpy(nparray1) # print(type(array_totensor))\n",
        "# tensor -> array\n",
        "tensor1 = torch.tensor([5,6,7,8])\n",
        "tensor_toarray = tensor1.detach().numpy() # print(type(tensor_toarray))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "M6XrMQL6kH1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2501f8fd-615f-40fd-c715-178df7b5dc88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2b. Pivoting Data**\n",
        "\n",
        "**NOTE:** Values can easily be modified by using the accessing method to select \n",
        "the desired section of the array/tensor to be modified.\n",
        "* Indexing is using the location of an element in an array/tensor to extract it.\n",
        "* Slicing is used to obtain/extract a subset of elements in an array/tensor.\n",
        "\n",
        "**NOTE:**  The size and shape of an array/tensor mean the same thing."
      ],
      "metadata": {
        "id": "Vgo3dkKT6lky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Flattening an array/tensor means to remove all of the dimensions except for one.\n",
        "\n",
        "# Flatten a Numpy Array\n",
        "original_array = np.random.randint(3, size = (2, 3, 4)) \n",
        "flattened_array = original_array.flatten()\n",
        "\n",
        "# Flatten a Torch Tensor\n",
        "original_tensor = torch.rand(size=(2, 3, 4))\n",
        "flattened_tensor = original_tensor.flatten()"
      ],
      "outputs": [],
      "metadata": {
        "id": "spZX1gtGltdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e0bb14-47f3-45f0-aa6a-6247e864e8e3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Squeezing an array/tensor removes the dimensions or axes that have a length of\n",
        "# one, while unsqueezing an array/tensor adds a dimension with a length of one.\n",
        "# These functions allow us to expand or shrink the rank (number of dimensions)\n",
        "# of our array/tensor.\n",
        "\n",
        "# Squeeze & unsqueeze a Numpy Array\n",
        "original_array = np.random.randint(3, size = (6, 1, 3))\n",
        "squeezed_array = np.squeeze(original_array, axis = 1)\n",
        "unsqueezed_array = np.expand_dims(squeezed_array, axis = 1) \n",
        "\n",
        "# Squeeze & unsqueeze a Pytorch Tensor\n",
        "original_tensor = torch.rand(size=(3, 2, 1, 2))\n",
        "squeezed_tensor = original_tensor.squeeze(2)\n",
        "unsqueezed_tensor = squeezed_tensor.unsqueeze(2)"
      ],
      "outputs": [],
      "metadata": {
        "id": "xdhYbZs3pLsR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0d772c-f11a-4db4-a180-7b87410e707c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Using the reshape() function, we can specify the shape that we are seeking, \n",
        "# but the number of elements remain unchanged in the array/tensor. \n",
        "# Numpy/Pytorch allow us to give one of new shape parameter as -1 (eg: (2,-1) or\n",
        "# (-1,3) but not (-1, -1)). It simply means that it is an unknown dimension and \n",
        "# we want numpy/pytorch to figure it out. Numpy/Pytorch will figure this by \n",
        "# looking at the 'length of the array and remaining dimensions' and making sure \n",
        "# it satisfies the above mentioned criteria\n",
        "\n",
        "# Reshaping a Numpy Array\n",
        "original_array = np.random.randint(3, size = (2, 3, 4)) \n",
        "reshaped_array1 = np.reshape(original_array, (4, 2, 3))\n",
        "reshaped_array2 = np.reshape(original_array, (6, -1))\n",
        "\n",
        "# Reshaping a Torch Tensor\n",
        "original_tensor = torch.rand(size=(3, 1, 2, 2))\n",
        "reshaped_tensor1 = original_tensor.reshape(3, 4, 1)\n",
        "reshaped_tensor2 = original_tensor.reshape(2, -1)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "fa9QEB7Xtl_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "576c29fa-04ac-4296-c090-82a1e2f4997c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# NUMPY: If the axes are specified, it must be a tuple or list which contains a \n",
        "# permutation of [0,1,..,N-1] where N is the number of axes of the array. The \n",
        "# i’th axis of the returned array will correspond to the axis numbered axes[i] \n",
        "# of the input. If not specified, defaults to range(a.ndim)[::-1], which \n",
        "# reverses the order of the axes.\n",
        "\n",
        "# Transposing a Numpy Array\n",
        "original_array = np.random.randint(3, size = (2, 3)) \n",
        "# Matrix Tranpose with axis left empty\n",
        "transposed_array1 = np.transpose(original_array) \n",
        "\n",
        "example_array = np.random.randint(3, size = (2, 3, 5))\n",
        "# axes tuple must be of size n-1 where n = rank of array (3 in this case)\n",
        "transposed_exampleX = np.transpose(example_array, axes = (0,1,2)) \n",
        "transposed_example1 = np.transpose(example_array, axes = (1,0,2)) \n",
        "transposed_example2 = np.transpose(example_array, axes = (2,1,0))\n",
        "transposed_example3 = np.transpose(example_array, axes = (1,0,2))\n",
        "transposed_example4 = np.transpose(example_array, axes = (1,2,0)) \n",
        "\n",
        "\n",
        "# PYTORCH: Returns a tensor that is a transposed version of input. The given \n",
        "# dimensions are swapped. The resulting tensor shares it’s underlying storage\n",
        "# with the input tensor, so changing the content of one would change the \n",
        "# content of the other.\n",
        "\n",
        "# Transposing a Torch Tensor\n",
        "original_tensor = torch.rand(size=(2, 3, 4))\n",
        "transposed_tensor1 = original_tensor.transpose(0,2)\n",
        "\n",
        "# **NOTE:** For Torch, the *permute* operation operation allows the user to \n",
        "# simultaneously reorder multiple dimensions unlike *transpose* which \n",
        "# interchanges two dimensions only.  PyTorch torch.permute() rearranges the \n",
        "# original tensor according to the desired ordering and returns a new \n",
        "# multidimensional rotated tensor. The size of the returned tensor remains the\n",
        "# same as that of the original.\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "Xhyrnt0T--xE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed920f32-1e44-411a-9767-8b87b21aed40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2c. Combining Data**"
      ],
      "metadata": {
        "id": "o2SjC78OXA6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# A concatenation operation joins a sequence of arrays/tensors along an existing\n",
        "# axis. All arrays/tensors must either have the same shape (except in the \n",
        "# concatenating dimension) or be empty.\n",
        "\n",
        "# Concatenating Numpy Arrays\n",
        "array1 = np.random.randint(3, size = (3, 4, 5))\n",
        "array2 = np.random.randint(4, size = (3, 4, 5))\n",
        "concatenated_array1 = np.concatenate((array1, array2), axis = 0) # (6, 4, 5)\n",
        "concatenated_array2 = np.concatenate((array1, array2), axis = 1) # (3, 8, 5)\n",
        "concatenated_array3 = np.concatenate((array1, array2), axis = 2) # (3, 4, 10)\n",
        "\n",
        "# Concatenating Torch Tensors\n",
        "tensor1 = torch.rand(size=(3, 4, 5))\n",
        "tensor2 = torch.rand(size=(3, 4, 5))\n",
        "concatenated_tensor1 = torch.cat([tensor1, tensor2], dim=0) # (6, 4, 5)\n",
        "concatenated_tensor2 = torch.cat([tensor1, tensor2], dim=1) # (3, 8, 5)\n",
        "concatenated_tensor3 = torch.cat([tensor1, tensor2], dim=2) # (3, 4, 10)"
      ],
      "outputs": [],
      "metadata": {
        "id": "uKohuN6ySITt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ffc5478-c1f9-4d06-8f32-fcdc2840d214"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# The stack operation joins a sequence of arrays/tensors along a new axis. The \n",
        "# axis parameter specifies the index of the new axis in the dimensions of the \n",
        "# result. For example, if axis=0 it will be the first dimension and if axis=-1 \n",
        "# it will be the last dimension. All arrays/tensors need to be of the same size.\n",
        "# The stacked array/tensor has one more dimension than the input arrays.\n",
        "\n",
        "# Stacking Numpy Arrays\n",
        "array1 = np.random.randint(3, size = (3, 4, 5))\n",
        "array2 = np.random.randint(4, size = (3, 4, 5))\n",
        "stacked_array1 = np.stack((array1, array2), axis = 0) # (2, 3, 4, 5)\n",
        "stacked_array2 = np.stack((array1, array2), axis = 1) # (3, 2, 4, 5)\n",
        "stacked_array3 = np.stack((array1, array2), axis = 2) # (3, 4, 2, 5)\n",
        "stacked_array4 = np.stack((array1, array2), axis = -1) # (3, 4, 5, 2)\n",
        "\n",
        "# Stacking Torch Tensors\n",
        "tensor1 = torch.rand(size=(3, 4, 5))\n",
        "tensor2 = torch.rand(size=(3, 4, 5))\n",
        "stacked_tensor1 = torch.stack([tensor1, tensor2], dim=0) # (2, 3, 4, 5)\n",
        "stacked_tensor2 = torch.stack([tensor1, tensor2], dim=1) # (3, 2, 4, 5)\n",
        "stacked_tensor3 = torch.stack([tensor1, tensor2], dim=2) # (3, 4, 2, 5)\n",
        "stacked_tensor4 = torch.stack([tensor1, tensor2], dim=-1) # (3, 4, 5, 2)\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "xB_4Zsx3pSdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deefb812-c8c9-4634-f00b-32c8c1d1db56"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# The repeat operation repeats elements of an array. The number of repetitions \n",
        "# for each element is broadcasted to fit the shape of the given axis.\n",
        "\n",
        "# Repeat in Numpy Arrays\n",
        "original_array = np.array([[1,2],[3,4]])\n",
        "repeated_array1 = np.repeat(original_array, 2) # (8, )\n",
        "repeated_array2 = np.repeat(original_array, 3, axis=0) # (6, 2)\n",
        "repeated_array3 = np.repeat(original_array, 3, axis=1) # (2, 6)\n",
        "repeated_array4 = np.repeat(original_array, [2,3], axis=0) # (5, 2)\n",
        "\n",
        "# In the Torch version of 'repeat', only the number of repeats can be specified,\n",
        "# and will be done along each dimension. This can, however, be done using \n",
        "# 'repeat_interleave'.\n",
        "\n",
        "# Repeat in Torch Tensors\n",
        "original_tensor = torch.tensor([1,2,3,4]) #(4)\n",
        "repeated_tensor1 = original_tensor.repeat((0)) #()\n",
        "repeated_tensor2 = original_tensor.repeat((2)) # (8)\n",
        "repeated_tensor3 = original_tensor.repeat((2,3)) #(2, 12)\n",
        "\n",
        "# Repeat Interleave in Torch Tensors\n",
        "original_tensor = torch.tensor([[1,2],[3,4]])\n",
        "repeated_tensor1 = original_tensor.repeat_interleave(2) #(8)\n",
        "repeated_tensor2 = original_tensor.repeat_interleave(3, dim=0) #(6, 2)\n",
        "repeated_tensor3 = original_tensor.repeat_interleave(3, dim=1) #(2, 6)"
      ],
      "outputs": [],
      "metadata": {
        "id": "FqXsl6LNxRBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7f61dd-0fd3-489a-eb4a-68c9cab1cc1e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Arrays/tensors need to be padded to ensure that computations can be optimized \n",
        "# by transfroming the underlying data to become of the same size.\n",
        "\n",
        "# Padding Numpy Arrays\n",
        "original_array = np.array([[1,2,3,4],\n",
        "                 [1,2,3,4],\n",
        "                 [1,2,3,4],\n",
        "                 [1,2,3,4]])\n",
        "# Setting the width of padding for each side\n",
        "pad_left   = 1\n",
        "pad_right  = 2\n",
        "pad_top    = 1\n",
        "pad_bottom = 2\n",
        "\n",
        "padded_array1 = np.pad(original_array, pad_width =  ((pad_top, pad_bottom), (pad_left, pad_right)), mode = 'constant' )\n",
        "padded_array2 = np.pad(original_array, pad_width =  ((pad_top, pad_bottom), (pad_left, pad_right)), mode = 'edge' )\n",
        "padded_array3 = np.pad(original_array, pad_width =  ((pad_top, pad_bottom), (pad_left, pad_right)), mode = 'reflect' )\n",
        "padded_array4 = np.pad(original_array, pad_width =  ((pad_top, pad_bottom), (pad_left, pad_right)), mode = 'symmetric')\n",
        "\n",
        "# Padding Torch Tensors\n",
        "# NOTE: Requires special package from torch.nn\n",
        "from torch.nn import functional as F\n",
        "original_tensor = torch.tensor([[1,2,3,4],\n",
        "                 [1,2,3,4],\n",
        "                 [1,2,3,4],\n",
        "                 [1,2,3,4]])\n",
        "padded_tensor1 = F.pad( original_tensor, (pad_left, pad_right, pad_top, pad_bottom), mode = 'constant' )"
      ],
      "outputs": [],
      "metadata": {
        "id": "PSo_CfNuDyw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be19ee29-c517-45f7-9c58-2655dd9ffdc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2d. Mathematical Operations**"
      ],
      "metadata": {
        "id": "LU1a6CgxiHwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Point-wise/element-wise Array operations**\n",
        "* Addition/Multiplication with Scalars\n",
        "* Elementwise Addition/Multiplication (aka Hadmard Product) of Arrays\n",
        "* Absolute value\n",
        "* Broadcasting b/w arrays of different dimensions\n",
        "*Note:* When broadting two multi-dimensional tensors, match their corresponding \n",
        "dimensions beginning from the last dimension.\n",
        "All dimensions should either match or one of the arrays should have length 1 \n",
        "in that specific dimension\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reduction Operations**\n",
        "\n",
        "NumPy & Torch support all commonly used mathematical reduction operations such \n",
        "as sum(), mean(), std(), max(), argmax(), unique() etc. These can either be \n",
        "applied on the entire array/tensor or along specific dimensions."
      ],
      "metadata": {
        "id": "kAc6B3Yhk2Tf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison Operations**\n",
        "\n",
        "Comparison Operations preform comparision on the array/tensors as a whole as \n",
        "well as along particular axes."
      ],
      "metadata": {
        "id": "NQirKU_FnyCS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Numpy Comparison Operations\n",
        "original_array1 = np.random.randint(3, size=(3,4))\n",
        "original_array2 = np.random.randint(3, size=(3,4))\n",
        "original_array3 = np.random.randint(3, size=(3,4))\n",
        "\n",
        "# Element-wise Comparison Operations for > < or !=\n",
        "# Combining reduction operations with boolean tensors\n",
        "print((original_array1 > original_array2).any(), \"\\n\") # ||\n",
        "print((original_array1 > original_array2).all(), \"\\n\") # &&\n",
        "print((original_array1 > original_array2).any(axis=0), \"\\n\")\n",
        "\n",
        "# Torch Comparison Operations\n",
        "original_tensor1 = torch.rand(size=(3,4))\n",
        "original_tensor2 = torch.rand(size=(3,4))\n",
        "original_tensor3 = torch.rand(size=(3,4))\n",
        "\n",
        "# Element-wise Comparison Operations for > < or !=\n",
        "# Combining reduction operations with boolean tensors\n",
        "print((original_tensor1 > original_tensor2).any(), \"\\n\") # ||\n",
        "print((original_tensor1 > original_tensor2).all(), \"\\n\") # &&\n",
        "print((original_tensor1 > original_tensor2).any(axis=0), \"\\n\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "zDrVexELsjkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1a1f95-67be-4b04-a9c5-136d484c96bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector/Matrix Operations**\n",
        "\n",
        "**Dot Product:** aka Inner product (Matrix multiplication relies on dot product to multiply various combinations of rows and columns.)\n",
        "\n",
        "**Tensor Product:** Tensordot (also known as tensor contraction) sums the product of elements from a and b over the indices specified by a_axes and b_axes. The lists a_axes and b_axes specify those pairs of axes along which to contract the tensors. (np.tensordot(), torch.tensordot())\n",
        "\n",
        "**Einsum:** Imagine that we have two multi-dimensional arrays, A and B. Now let's suppose we want to... multiply A with B in a particular way to create new array of products; and then maybe sum this new array along particular axes; and then maybe transpose the axes of the new array in a particular order.\n",
        "There's a good chance that einsum will help us do this faster and more memory-efficiently that combinations of the NumPy functions like multiply, sum and transpose will allow. (np.einsum(), torch.einsum())"
      ],
      "metadata": {
        "id": "yGq0WuNKuOCf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Numpy Vector/Matrix operations\n",
        "array1 = np.random.randn(3)\n",
        "array2 = np.random.randn(3)\n",
        "array3 = np.random.randn(3, 4)\n",
        "array4 = np.random.randn(4)\n",
        "matrix1 = np.random.randint(4, size = (2, 3))\n",
        "matrix2 = np.random.randint(4, size = (3, 2))\n",
        "# Matmul Examples \n",
        "# Vector x Vector\n",
        "print('Matmul: \\n', np.matmul(array1, array2))\n",
        "print(\"Matmul: \\n\", array1@array2)\n",
        "# Matrix x Vector\n",
        "print('Matmul: \\n', np.matmul(array3, array4))\n",
        "print('Matmul: \\n', array3@array4)\n",
        "# Matrix x Matrix \n",
        "print('Matmul: \\n', np.matmul(matrix1, matrix2))\n",
        "\n",
        "\n",
        "# Torch Vector/Matrix operations\n",
        "tensor1 = torch.randn(3)\n",
        "tensor2 = torch.randn(3)\n",
        "tensor3 = torch.randn(3, 4)\n",
        "tensor4 = torch.randn(4)\n",
        "matrix1 = torch.randn(2, 3)\n",
        "matrix2 = torch.randn(3, 2)\n",
        "# Vector x Vector\n",
        "print('Matmul: \\n', torch.matmul(tensor1, tensor2))\n",
        "# Matrix x Vector\n",
        "print('Matmul: \\n', torch.matmul(tensor3, tensor4))\n",
        "# Matrix x Matrix\n",
        "print('Matmul: \\n', torch.matmul(matrix1, matrix2))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "Cd-HGMpAxjKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "119278ec-99d0-4d69-b1fc-a299fb074686"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Torch GPU operation**\n",
        "\n",
        "Pytorch’s tensors is able to perform operations on the GPU as opposed to the \n",
        "CPU. To do GPU tensor operations, you must first move the tensor from the CPU \n",
        "to the GPU. Operations require all components to be on the same device (CPU or\n",
        "GPU). Operations between CPU and GPU tensors will fail. A convenient method \n",
        "is `new`, which creates a new tensor on the same device as another tensor. \n",
        "It should be used for creating tensors whenever possible.\n",
        "\n",
        "A GPU operation’s runtime comes in two parts: 1) time taken to move a tensor to \n",
        "GPU, 2) time taken for an operation. #2 is very fast on GPU, but sometimes (for \n",
        "small operations), #1 can take much longer. In some cases, it may be faster to \n",
        "perform a certain operation on CPU. GPU memory is quite limited. You will \n",
        "frequently run into the following error. First is the RuntimeError, CUDA out of \n",
        "memory. When this happens, either reduce the batch size or check if there are \n",
        "any dangling unused tensors left on the GPU. You can delete tensors on the GPU \n",
        "and free memory with ```torch.cuda.empty_cache()```. You’ll be running into Cuda\n",
        "errors like: RuntimeError: CUDA error: device-side assert triggered. This can \n",
        "mean many things. For example, an operation between CPU and GPU tensors, GPU \n",
        "operations between tensors of unexpected shape, wrong types wrong in some weird\n",
        "way. You should try running the entire thing again after setting the following \n",
        "environment variable: ```CUDA_LAUNCH_BLOCKING=1```, which forces CUDA to do \n",
        "things sequentially• Remember to turn this back to ```CUDA_LAUNCH_BLOCKING=0```\n",
        "after. Otherwise your code will be slow.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# import torch\n",
        "a = torch.randn(5, 5)\n",
        "\n",
        "# Put tensor on CUDA if available\n",
        "x = torch.rand(3,2)\n",
        "if torch.cuda.is_available():\n",
        "    x = x.to(\"cuda:0\")\n",
        "    print(x, x.dtype)\n",
        "    \n",
        "# Do some calculations\n",
        "y = x ** 2 \n",
        "print(y)\n",
        "\n",
        "# Copy to CPU if on GPU\n",
        "if y.is_cuda:\n",
        "    y = y.cpu()\n",
        "    print(y, y.dtype)\n",
        "\n",
        "try:\n",
        "    a.cuda()\n",
        "except AssertionError as e:\n",
        "    print(e)\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Torch Automatic differentiation**\n",
        "Tensors provide automatic differentiation, and Tensors you are differentiating \n",
        "with respect to must have `requires_grad=True`. Call `.backward()` on scalar \n",
        "variables you are differentiating. To differentiate a vector, sum it first. \n",
        "Differentiation accumulates gradients. This is sometimes what you want and \n",
        "sometimes not. Make sure to zero gradients between batches if performing \n",
        "gradient descent or you will get strange results!\n",
        "\n",
        "Pytorch remembers the graph of all computations to perform differenciation. To \n",
        "be integrated to this graph the raw data is wrapped internally to the Tensor \n",
        "class (like what was formerly a Variable). You can detach the tensor from the \n",
        "graph using the **.detach()** method, which returns a tensor with the same data \n",
        "but requires_grad set to False. We can also set flag `requires_grad = False`, \n",
        "which do note update the graph. If you are in a context where you have a \n",
        "differentiable tensor that you don't need to differentiate, think of detaching \n",
        "it from the graph."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create differentiable tensor\n",
        "# x=torch.tensor(torch.arange(0,4), requires_grad=True, dtype=torch.float)\n",
        "# Surpress warning\n",
        "x = torch.arange(0,4).float()\n",
        "x.requires_grad = True\n",
        "y = x**2\n",
        "# Calculate gradient (dy/dx=2x)\n",
        "y.sum().backward() \n",
        "# Print values\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.grad)\n",
        "\n",
        "# Differentiate again (accumulates gradient)\n",
        "torch.sum(x**2).backward()\n",
        "print(x.grad)\n",
        "# Zero gradient before differentiating\n",
        "x.grad.data.zero_()\n",
        "torch.sum(x**2).backward()\n",
        "print(x.grad)\n",
        "x.detach().numpy()"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Torch Neural Net Example Using MNIST**\n",
        "\n",
        "torch.nn provides basic 1-layer nets, such as Linear (perceptron) and activation\n",
        "layers. All nn.Module objects are reusable as components of bigger networks! \n",
        "That is how you build personnalized nets. The simplest way is to use the \n",
        "nn.Sequential class. You can also create your own class that inherits n.Module. \n",
        "The forward method should precise what happens in the forward pass given an \n",
        "input. This enables you to precise behaviors more complicated than just applying\n",
        "layers one after another, if necessary.\n",
        "\n",
        "Parameters are of type Parameter, which is basically a wrapper for a tensor. \n",
        "They are the attributes of type Parameter in your network. Moreover, if an \n",
        "attribute is of type nn.Module, its own parameters are added to your network's \n",
        "parameters. Hence, when defining a network by adding up basic components such \n",
        "as `nn.Linear`, you should never have to explicitely define parameters. However,\n",
        "if you are in a case where no pytorch default module does what you need, you can\n",
        "define parameters explicitely (this should be rare). Parameters are meant to be \n",
        "all the network's weights that will be optimized during training. If you were \n",
        "needing to use a tensor in your computational graph that you want to remain \n",
        "constant, just define it as a regular tensor.\n",
        "\n",
        "`nn.CrossEntropyLoss` does both the softmax and the actual cross-entropy: given \n",
        "$output$ of size $(n,d)$ and $y$ of size $n$ and values in $0,1,...,d-1$, it \n",
        "computes $\\sum_{i=0}^{n-1}log(s[i,y[i]])$ where \n",
        "$s[i,j] = \\frac{e^{output[i,j]}}{\\sum_{j'=0}^{d-1}e^{output[i,j']}}$. You can \n",
        "also compose nn.LogSoftmax and nn.NLLLoss to get the same result. Note that all \n",
        "these use the log-softmax rather than the softmax, for stability in the \n",
        "computations.\n",
        "\n",
        "Now, to perform the backward pass, just execute **loss.backward()** ! It will \n",
        "update gradients in all differentiable tensors in the graph, which in particular\n",
        "includes all the network parameters.\n",
        "\n",
        "The MNIST dataset is a large database of handwritten digits that is commonly \n",
        "used for training various image processing systems. The database is also widely\n",
        "used for training and testing in the field of machine learning. It consists of \n",
        "60,000 training images and 10,000 testing images."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "cuda = torch.cuda.is_available() # Define if awailable\n",
        "\n",
        "torch.__version__\n",
        "\n",
        "# Simple MLP with 2 layers and sigmoid activation using sequential network \n",
        "# (`nn.Module` object) from layers (other `nan.Module` objects).\n",
        "\n",
        "x = torch.arange(0,32).float() \n",
        "# net = torch.nn.Sequential(\n",
        "#     torch.nn.Linear(32,128),\n",
        "#     torch.nn.Sigmoid(),\n",
        "#     torch.nn.Linear(128,10))\n",
        "# y = net(x)\n",
        "# print(y)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Obtain related dataset, MNIST\n",
        "train = MNIST('./data', train=True, download=False, transform=transforms.ToTensor())\n",
        "test = MNIST('./data', train=False, download=False, transform=transforms.ToTensor())\n",
        "train_data = train.data\n",
        "train_data = train.transform(train_data.numpy())\n",
        "\n",
        "print('[Train Data]')\n",
        "print(' - Numpy Shape:', train_data.cpu().numpy().shape)\n",
        "print(' - Tensor Shape:', train_data.size())\n",
        "print(' - min:', torch.min(train_data))\n",
        "print(' - max:', torch.max(train_data))\n",
        "print(' - mean:', torch.mean(train_data))\n",
        "print(' - std:', torch.std(train_data))\n",
        "print(' - var:', torch.var(train_data))\n",
        "\n",
        "print('\\n[Train Labels]')\n",
        "print(' - Numpy Shape:', train.targets.cpu().numpy().shape)\n",
        "print(' - Tensor Shape:', train.targets.size())\n",
        "\n",
        "# plt.imshow(train.train_data.cpu().numpy()[1], cmap='gray')\n",
        "# print (train.train_labels.cpu().numpy()[1])"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Self-defined data loader\n",
        "class MyDataset(data.Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        X = self.X[index].float().reshape(-1) #flatten the input\n",
        "        Y = self.Y[index].long()\n",
        "        return X,Y"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# create a more customizable network module (equivalent here)\n",
        "class MyNetwork(torch.nn.Module):\n",
        "    # you can use the layer sizes as initialization arguments if you want to\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = torch.nn.Linear(input_size,hidden_size)\n",
        "        self.layer2 = torch.nn.Sigmoid()\n",
        "        self.layer3 = torch.nn.Linear(hidden_size,output_size)\n",
        "\n",
        "    def forward(self, input_val):\n",
        "        h = input_val\n",
        "        h = self.layer1(h)\n",
        "        h = self.layer2(h)\n",
        "        h = self.layer3(h)\n",
        "        return h\n",
        "\n",
        "class MyNetworkWithParams(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MyNetworkWithParams,self).__init__()\n",
        "        self.layer1_weights = nn.Parameter(torch.randn(input_size,hidden_size))\n",
        "        self.layer1_bias = nn.Parameter(torch.randn(hidden_size))\n",
        "        self.layer2_weights = nn.Parameter(torch.randn(hidden_size,output_size))\n",
        "        self.layer2_bias = nn.Parameter(torch.randn(output_size))\n",
        "        \n",
        "    def forward(self,x):\n",
        "        h1 = torch.matmul(x,self.layer1_weights) + self.layer1_bias\n",
        "        h1_act = torch.max(h1, torch.zeros(h1.size())) # ReLU\n",
        "        output = torch.matmul(h1_act,self.layer2_weights) + self.layer2_bias\n",
        "        return output\n",
        "\n",
        "class MyNetWithMultiHidenLayer(nn.Module):\n",
        "    def __init__(self,n_hidden_layers):\n",
        "        super(MyNet,self).__init__()\n",
        "        self.n_hidden_layers=n_hidden_layers\n",
        "        self.final_layer = nn.Linear(128,10)\n",
        "        self.act = nn.ReLU()\n",
        "        self.hidden = []\n",
        "        for i in range(n_hidden_layers):\n",
        "            self.hidden.append(nn.Linear(128,128))\n",
        "    \n",
        "    def forward(self,x):\n",
        "        h = x\n",
        "        for i in range(self.n_hidden_layers):\n",
        "            h = self.hidden[i](h)\n",
        "            h = self.act(h)\n",
        "        out = self.final_layer(h)\n",
        "        return out\n",
        "\n",
        "# SIMPLE MODEL DEFINITION\n",
        "class Simple_MLP(nn.Module):\n",
        "    def __init__(self, size_list):\n",
        "        super(Simple_MLP, self).__init__()\n",
        "        layers = []\n",
        "        self.size_list = size_list\n",
        "        for i in range(len(size_list) - 2):\n",
        "            layers.append(nn.Linear(size_list[i],size_list[i+1]))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(size_list[-2], size_list[-1]))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    start_time = time.time()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
        "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
        "        data = data.to(device)\n",
        "        target = target.to(device) # all data & model on same device\n",
        "\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    end_time = time.time()\n",
        "    \n",
        "    running_loss /= len(train_loader)\n",
        "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
        "    return running_loss\n",
        "\n",
        "def test_model(model, test_loader, criterion):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        total_predictions = 0.0\n",
        "        correct_predictions = 0.0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            outputs = model(data)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_predictions += target.size(0)\n",
        "            correct_predictions += (predicted == target).sum().item()\n",
        "\n",
        "            loss = criterion(outputs, target).detach()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "        running_loss /= len(test_loader)\n",
        "        acc = (correct_predictions/total_predictions)*100.0\n",
        "        print('Testing Loss: ', running_loss)\n",
        "        print('Testing Accuracy: ', acc, '%')\n",
        "        return running_loss, acc"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "num_workers = 8 if cuda else 0 \n",
        "    \n",
        "# Training\n",
        "train_dataset = MyDataset(train.data, train.targets)\n",
        "train_loader_args = dict(shuffle=True, batch_size=256, \\\n",
        "                         num_workers=num_workers, pin_memory=True) \\\n",
        "    if cuda else dict(shuffle=True, batch_size=64)\n",
        "# Testing\n",
        "test_dataset = MyDataset(test.test_data, test.test_labels)\n",
        "\n",
        "test_loader_args = dict(shuffle=False, batch_size=256, \\\n",
        "                        num_workers=num_workers, pin_memory=True) \\\n",
        "    if cuda else dict(shuffle=False, batch_size=1)\n",
        "\n",
        "train_loader = data.DataLoader(train_dataset, **train_loader_args)\n",
        "test_loader = data.DataLoader(test_dataset, **test_loader_args)\n",
        "\n",
        "n_epochs = 10\n",
        "Train_loss = []\n",
        "Test_loss = []\n",
        "Test_acc = []\n",
        "\n",
        "model = Simple_MLP([784, 256, 20])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "model.to(device)\n",
        "print(model)\n",
        "for i in range(n_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    test_loss, test_acc = test_model(model, test_loader, criterion)\n",
        "    Train_loss.append(train_loss)\n",
        "    Test_loss.append(test_loss)\n",
        "    Test_acc.append(test_acc)\n",
        "    print('='*20)\n",
        "\n",
        "# save a dictionary\n",
        "torch.save(model.state_dict(),'test.t7')\n",
        "# load a dictionary\n",
        "model.load_state_dict(torch.load('test.t7'))\n",
        "\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(Train_loss)\n",
        "\n",
        "plt.title('Test Loss')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(Test_loss)\n",
        "\n",
        "plt.title('Test Accuracy')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.plot(Test_acc)\n"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {}
    }
  ]
}